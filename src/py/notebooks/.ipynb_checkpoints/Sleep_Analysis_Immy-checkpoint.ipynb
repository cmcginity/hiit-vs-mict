{
<<<<<<< Updated upstream
 "cells": [],
 "metadata": {},
=======
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7980123b-3e9a-4f92-8869-63fbf4c03ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': '_realtime, _time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m dfsl_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname, fid \u001b[38;5;129;01min\u001b[39;00m fileids\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 62\u001b[0m     dfsl \u001b[38;5;241m=\u001b[39m load_data_from_drive(drive_service, fid)\n\u001b[1;32m     63\u001b[0m     dfsl_list\u001b[38;5;241m.\u001b[39mappend(dfsl)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(dfsl_list\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[9], line 53\u001b[0m, in \u001b[0;36mload_data_from_drive\u001b[0;34m(_drive_service, file_id)\u001b[0m\n\u001b[1;32m     51\u001b[0m     status, done \u001b[38;5;241m=\u001b[39m downloader\u001b[38;5;241m.\u001b[39mnext_chunk()\n\u001b[1;32m     52\u001b[0m io_buffer\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(io_buffer, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_realtime\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:161\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(\n\u001b[1;32m    156\u001b[0m             usecols,\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_parse_dates_presence(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames)  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_noconvert_columns()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:230\u001b[0m, in \u001b[0;36mParserBase._validate_parse_dates_presence\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m    220\u001b[0m missing_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    222\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing column provided to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_dates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    235\u001b[0m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[1;32m    237\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: Missing column provided to 'parse_dates': '_realtime, _time'"
     ]
    }
   ],
   "source": [
    "# Google Drive Access:\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# Import packages for connecting to Google Drive\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "# from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "# CHANGE PATH NAME to where the service account credential is on your computer.\n",
    "credentials_path = '/Users/imogengardiner/Documents/exercise_credential.json'\n",
    "\n",
    "# REPLACE FOLDER_ID with the folder in Google Drive you are interested in importing data from.\n",
    "    # Instructions: Search online for the HIIT and Endurance Study shared google drive. Navigate to the folder with data you are interested in. \n",
    "    # Look at the web URL. Replace 'folder_id' with the end of the URL.\n",
    "    # Example for getting Polar Data: HIIT and Endurance Study Drive > Data > data > workout folder. The workout folder URL is https://drive.google.com/drive/folders/1a9Bmg89_9m9BaYLsLw52PS_m07FQ7CtZ.\n",
    "    # Notice how the end of the web URL is the folder_id.\n",
    "folder_id = '1PjDaMLxn3QvYqICxCxK5PF2-yhh8sNoa'\n",
    "\n",
    "# Authenticate with Google Drive using service account credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# Get files\n",
    "def get_file_ids_from_dir(parent_id):\n",
    "    # drive_service = setup_drive()\n",
    "    results = drive_service.files().list(\n",
    "        corpora='drive',\n",
    "        driveId='0AB6End4Uf7P-Uk9PVA', # if you search for the HIIT and Endurance Study shared google drive, this ID is taken from the end of the pathname: https://drive.google.com/drive/folders/0AB6End4Uf7P-Uk9PVA\n",
    "        q=f\"'{parent_id}' in parents\",\n",
    "        includeItemsFromAllDrives=True,\n",
    "        supportsAllDrives=True\n",
    "    ).execute()\n",
    "    files = results.get('files', [])\n",
    "    if not files:\n",
    "        raise Exception(f\"Folder {parent_id} has no files!\")\n",
    "    # id_name = [{x['name'] : x['id']} for x in files]\n",
    "    id_name = {}\n",
    "    for x in files:\n",
    "        id_name[x['name']] = x['id']\n",
    "    return id_name\n",
    "\n",
    "def load_data_from_drive(_drive_service, file_id):\n",
    "    request = _drive_service.files().get_media(fileId=file_id)\n",
    "    io_buffer = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(io_buffer, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "    io_buffer.seek(0)\n",
    "    df = pd.read_csv(io_buffer, parse_dates=['_realtime','_time'])\n",
    "    return df\n",
    "\n",
    "def preload_data_from_drive2(parent_id):\n",
    "    drive_service = setup_drive()\n",
    "    preloaded_data = {}\n",
    "    \n",
    "    # Get list of files in the specified folder\n",
    "    response = drive_service.files().list(corpora='drive', \n",
    "                                          driveId=fid_root,\n",
    "                                          q=f\"'{parent_id}' in parents\",\n",
    "                                          includeItemsFromAllDrives=True, \n",
    "                                          supportsAllDrives=True).execute()\n",
    "    # Iterate over each file and load its data\n",
    "    for file in response.get('files', []):\n",
    "        if file.get('name').startswith(\"q\"):\n",
    "            continue\n",
    "        file_id = file['id']\n",
    "        df = load_data_from_drive(drive_service, file_id)\n",
    "        preloaded_data[file_id] = df\n",
    "    \n",
    "    return preloaded_data\n",
    "\n",
    "# Get file names from folder\n",
    "fileids = get_file_ids_from_dir(folder_id)\n",
    "#print(fileids)\n",
    "\n",
    "dfsl_list = []\n",
    "for fname, fid in fileids.items():\n",
    "    dfsl = load_data_from_drive(drive_service, fid)\n",
    "    dfsl_list.append(dfsl)\n",
    "\n",
    "print(dfsl_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df33b4-8bc1-43e0-9353-c7a4f9e0ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer code from 007 analysis and loop through every file in the sl/Fitbit folder, and create plots for each participant:\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for file in dfsl_list:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Store the DataFrame in the dictionary with the file name as the key\n",
    "    dfs[file[:3]] = df\n",
    "\n",
    "# Drop unnecessary columns from the DataFrame\n",
    "for i in range(len(dfsl_list)):\n",
    "    columns_to_remove = ['device', 'start_date', 'end_date', 'start_time', 'end_time', '_time', 'myphd_id', 'target_hr_45', 'target_hr_55', 'target_hr_70', 'target_hr_90', 'ppt_id', 'moy_abbr', 'tag', 'datatype', 'sourcetype', 'dname',\n",
    "                    'myphd_date_shift']\n",
    "    dfsl_list[i] = dfsl_list[i].drop(columns=columns_to_remove)\n",
    "    \n",
    "dfsl_list = dfsl_list[dfsl_list['enrollment_status']=='Completed']\n",
    "                         \n",
    "\n",
    "print(dfsl_list.head())\n",
    "\n",
    "# Adjust the values to reflect seconds (rather than decaseconds)\n",
    "dfsl_list_004['value'] = dfsl_list_004['value']*10\n",
    "\n",
    "# Rename value to seconds\n",
    "dfsl_list_004 = dfsl_list_004.rename(columns={'value': 'seconds'})\n",
    "\n",
    "print(dfsl_list_004.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9d3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700ad0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
>>>>>>> Stashed changes
 "nbformat": 4,
 "nbformat_minor": 5
}
