{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polar Json converter\n",
    "\n",
    "Last updated by AKR (8/13/24).\n",
    "\n",
    "This notebook converts \"manually exported\" Polar json files into csv files.\n",
    "\n",
    "**Format of csv file output:**\n",
    "\n",
    "**f\"{participant id}\\_{polar workout number (pwo)}\\_{day since start date}_hr_WearablePolar_Polar.csv\"**\n",
    "\n",
    "For example: **019_pwo04_d006_hr_WearablePolar_Polar.csv** is participant 19's 4th workout, completed 6 days after the study start date.\n",
    "\n",
    "**Details:**\n",
    "- **pwo (polar workout number)**: workouts are counted if they are >5 minutes in duration. After passing this filter, they are numbered sequentially by date.\n",
    "- **d (days)**: workouts are named in reference to the \"study start date,\" defined as the the first recorded date in the Week 1 Workout Log in REDCap. This annotation in REDCap is the \"ground truth\" for when participants began exercising. Oftentimes, they test the device before the study or complete a workout session at the CTRU before the intervention. This will appear as negative days (e.g. 4 days before study is -d004). In other words, the first Polar entry may be a \"test\" or CTRU workout, so REDCap is the best and closest annotation to training start date.\n",
    "\n",
    "\n",
    "Each csv file represents an individual workout, with columns for dateTime and (HR) values. \n",
    "\n",
    "We expect participants to have 36 workouts (pwo01 - pwo36) if they have completed 3 workouts per week for 12 weeks. On average, participants would be expected to have participated for ~84 days in the study (i.e. 12 weeks), but they are allowed 2 week extensions for non-adherence, illness, or to accomodate for scheduling. In rare cases of injury, participants were extended beyond 14 weeks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json \n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#initialization\n",
    "random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['003_qtz1b14715581577341838', '004_qtz1b18445579426718188', '005_qtz1b16767772887265127', '006_qtz1b13893369732763681', '010_qtz1b19121547781291613', '012_qtz1b17269433695528197', '014_qtz1b13168362397931536', '018_qtz1b19261322566215748', '019_qtz1b17236523485537658', '020_qtz1b19129928133897239', '021_qtz1b16327775253853113', '030_qtz1b17855938635599315', '036_qtz1b18341388618579643', '037_qtz1b19851346611142986', '040_qtz1b12182259692885175', '045_qtz1b14361342418376156', '046_qtz1b12637499885254663', '047_qtz1b11649673216856592', '051_qtz1b16917131625167555', '060_qtz1b13779232687428538', '064_qtz1b12351664185336321', '071_qtz1b19649447249479579', '072_qtz1b19779165843537516', '073_qtz1b17592513599164541', '075_qtz1b19516699726434684', '077_qtz1b11121669136412997', '080_qtz1b19331948298967664', '084_qtz1b11871696423624282', '086_qtz1b11984351926876373', '087_qtz1b19955871415948163', '089_qtz1b15536926332385566', '090_qtz1b15741679182868673', '092_qtz1b15669559847457547', '094_qtz1b15347975827571443', '095_qtz1b16816367966245858', '097_qtz1b15382499214278169', '100_qtz1b13231144819788317', '105_qtz1b19188262295493232', '109_qtz1b19623656583828428', '110_qtz1b18354298432789339', '114_MISSINGID', '117_qtz1b17244974925687853', '123_qtz1b19516546253659856', '124_MISSINGID', '127_qtz1b17474444729879193', '129_qtz1b17466657167865448', '132_MISSING', '136_qtz1b18581229263346543', '1070_qtz1b14637549187739211', '1091_qtz1b12644415582713666', '1103_qtz1b14851388475823521', '1112_qtz1b12264939235647859', '1116_qtz1b16294177813262374', 'pl_rec_drop_075', 'hr']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "json_path = \"/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/\"\n",
    "\n",
    "# List only directories (folders)\n",
    "folders = [f for f in os.listdir(json_path) if os.path.isdir(os.path.join(json_path, f))]\n",
    "\n",
    "# Extract leading numbers and sort accordingly\n",
    "folders_sorted = sorted(folders, key=lambda x: int(re.match(r\"(\\d+)\", x).group()) if re.match(r\"(\\d+)\", x) else float('inf'))\n",
    "\n",
    "print(folders_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to gets files for a participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/014_qtz1b13168362397931536/polar-user-data-export_014_qtz1b13168362397931536/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#get_files(\"060_qtz1b13779232687428538\")\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mget_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m014_qtz1b13168362397931536\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m convert_polar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m014_qtz1b13168362397931536\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m, in \u001b[0;36mget_files\u001b[0;34m(participant)\u001b[0m\n\u001b[1;32m      2\u001b[0m json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparticipant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/polar-user-data-export_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparticipant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#json_path = f\"/Users/aubreykr/Library/CloudStorage/GoogleDrive-aubreykr@stanford.edu/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/097_qtz1b15382499214278169/polar-user-data-export_097_qtz1b15382499214278169/\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#json_path = f\"/Users/aubreykr/Library/CloudStorage/GoogleDrive-aubreykr@stanford.edu/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/060_qtz1b13779232687428538/polar-user-data-export_060_qtz1b13779232687428538/\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining-session*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/014_qtz1b13168362397931536/polar-user-data-export_014_qtz1b13168362397931536/'"
     ]
    }
   ],
   "source": [
    "def get_files(participant):\n",
    "    json_path = f\"/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/{participant}/polar-user-data-export_{participant}/\"\n",
    "    #json_path = f\"/Users/aubreykr/Library/CloudStorage/GoogleDrive-aubreykr@stanford.edu/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/097_qtz1b15382499214278169/polar-user-data-export_097_qtz1b15382499214278169/\"\n",
    "    #json_path = f\"/Users/aubreykr/Library/CloudStorage/GoogleDrive-aubreykr@stanford.edu/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/060_qtz1b13779232687428538/polar-user-data-export_060_qtz1b13779232687428538/\"\n",
    "    os.chdir(json_path)\n",
    "    files = []\n",
    "    for file in glob.glob(\"training-session*.json\"):\n",
    "        files.append(file)\n",
    "    original_files = files.copy()\n",
    "    files = sorted(files)\n",
    "    print(\"There are\", len(files), \"files for\", participant)\n",
    "    return files\n",
    "    \n",
    "#get_files(\"060_qtz1b13779232687428538\")\n",
    "\n",
    "get_files(\"014_qtz1b13168362397931536\")\n",
    "\n",
    "convert_polar(\"014_qtz1b13168362397931536\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert Polar json to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'[' was never closed (440295642.py, line 73)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 73\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(df_notes.loc[(df_notes['redcap_event_name'])\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '[' was never closed\n"
     ]
    }
   ],
   "source": [
    "def convert_polar(participant):\n",
    "\n",
    "    # 1) REDCAP DATA\n",
    "    \n",
    "    # Load participant workout logs from REDCap (goal: obtain workout dates from handwritten logs to compare with Polar wearable data).\n",
    "    notes_path = '/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/Redcap/Raw_Data/64208_WorkoutTimeStamps_Raw.csv'\n",
    "    df_notes_orig = pd.read_csv(f\"{notes_path}\")\n",
    "    df_notes= df_notes_orig.copy()\n",
    "\n",
    "    # Convert all workout dates to datetime\n",
    "    df_notes['workout_1'] = pd.to_datetime(df_notes['workout_1']) \n",
    "    df_notes['workout_2'] = pd.to_datetime(df_notes['workout_2'])\n",
    "    df_notes['workout_3'] = pd.to_datetime(df_notes['workout_3'])\n",
    "    df_notes['workout1_date_only'] = df_notes['workout_1'].dt.strftime(\"%Y-%m-%d\")\n",
    "    df_notes['workout2_date_only'] = df_notes['workout_2'].dt.strftime(\"%Y-%m-%d\")\n",
    "    df_notes['workout3_date_only'] = df_notes['workout_3'].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "    # 2) Get POLAR from Google Drive\n",
    "    \n",
    "    json_path = f\"/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar_recovery/{participant}/polar-user-data-export_{participant}_*/\"\n",
    "    os.chdir(json_path)\n",
    "    files = []\n",
    "    for file in glob.glob(\"training-session*.json\"):\n",
    "        files.append(file)\n",
    "    original_files = files.copy()\n",
    "    files = sorted(files)\n",
    "    print(\"There are\", len(files), \"files for\", participant)\n",
    "\n",
    "    \n",
    "    # For each each Polar json file, extract HR data; then save as a csv file in Google Drive.\n",
    "    # Create loop to iterate through Polar json files\n",
    "    i=0\n",
    "    counter=0\n",
    "    for i, file in enumerate(files):\n",
    "\n",
    "        # Get full path to file\n",
    "        full_file_name = f\"{json_path}{file}\"\n",
    "\n",
    "        # Read json data from the file and store it as a string\n",
    "        with open(full_file_name, 'r', ) as file2read:\n",
    "            json_data = file2read.read()\n",
    "            file2read.close()\n",
    "\n",
    "        # Parse json into a data dictionary with json.loads()\n",
    "        data_dict = json.loads(json_data)\n",
    "\n",
    "        # Extract key of interest\n",
    "        exercises = data_dict['exercises']\n",
    "\n",
    "        # Get data from Polar file (e.g. extract values, convert to dataframe)\n",
    "        try:\n",
    "            df_hr = pd.DataFrame(exercises[0]['samples']['heartRate'])\n",
    "\n",
    "        # Print if there is an error but keep running\n",
    "        except (KeyError, IndexError) as e:\n",
    "            # Handle the case where 'exercises' is missing or empty\n",
    "            print(f\"The file {participant} might be empty! Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert time column to datetime type\n",
    "        df_hr['dateTime'] = pd.to_datetime(df_hr['dateTime'])\n",
    "\n",
    "    # 3) Enrich Polar file with REDCap workout annotations\n",
    "        \n",
    "        # a) Extract date of each workout as a string\n",
    "        date_info = df_hr['dateTime'].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "        workout_date = pd.to_datetime(date_info).date() # Convert string to date\n",
    "\n",
    "        # b) Get redcap start date for each ppt\n",
    "        participant_num = int(participant.split('_')[0]) # Get ppt number from polar file\n",
    "        start_date_str = df_notes.loc[(df_notes['redcap_event_name'] == 'week_1_arm_1') & (df_notes['record_id'] == participant_num), 'workout_1'].values[0]\n",
    "        start_date = pd.to_datetime(start_date_str).date() # Convert string to date\n",
    "\n",
    "        # c) Compute day of workout (d) relative to redcap start date \n",
    "        delta = workout_date - start_date\n",
    "        days_str = str(delta.days)\n",
    "        day = f'd{days_str:0>3}'\n",
    "\n",
    "        # d) If the workout is longer than 5 minutes, label it as a polar workout (pwo)\n",
    "        if df_hr['dateTime'].max() - df_hr['dateTime'].min() > timedelta(minutes=5):\n",
    "\n",
    "            # Update pwo number\n",
    "            counter += 1\n",
    "            workout_num = f'pwo{counter:02}'\n",
    "\n",
    "\n",
    "        # e) Rename file in desired format with participant ID, polar workout number (pwo), and day (d) since study start\n",
    "            wo_info = f'{workout_num}_{day}'\n",
    "            output_path = f\"/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/{participant}/\"\n",
    "            output_name = f\"{participant.split('_')[0]}_{wo_info}_hr_WearablePolar_Polar.csv\" \n",
    "            fullname = os.path.join(output_path, output_name)   \n",
    "\n",
    "\n",
    "        # f) Save to drive\n",
    "            if not os.path.exists(output_path):  \n",
    "                os.mkdir(output_path) \n",
    "\n",
    "            if not os.path.exists(fullname): \n",
    "                df_hr.to_csv(fullname, index=False)\n",
    "                print(f\"CSV file saved: {fullname}\")\n",
    "            \n",
    "            # If files already exist, don't overwrite!\n",
    "            else:\n",
    "                print(f\"File already exists and will not be overwritten.\") \n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "    print(f'Participant {participant_num}: {counter} workouts logged.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 files for 003_qtz1b14715581577341838\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo01_d000_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo02_d001_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo03_d003_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo04_d007_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo05_d008_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo06_d010_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo07_d016_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo08_d017_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo09_d023_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo10_d024_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo11_d025_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo12_d027_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo13_d029_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo14_d030_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo15_d035_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo16_d037_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo17_d039_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo18_d049_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo19_d051_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo20_d052_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo21_d054_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo22_d096_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo23_d096_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo24_d096_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo25_d097_hr_WearablePolar_Polar.csv\n",
      "CSV file saved: /Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/003_qtz1b14715581577341838/003_pwo26_d099_hr_WearablePolar_Polar.csv\n",
      "Processing complete.\n",
      "Participant 3: 26 workouts logged.\n"
     ]
    }
   ],
   "source": [
    "convert_polar(\"003_qtz1b14715581577341838\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the QC check files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the QC check files\n",
    "participant = \"073_qtz1b17592513599164541\"\n",
    "base_dir = f\"/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/polar/workout/{participant}/\"\n",
    "\n",
    "# Load redcap data\n",
    "df_path = '/Users/aubreykr/Google Drive/Shared drives/HIIT and Endurance Study/Data/data/Redcap/Raw_Data/64208_HRZones_Raw.csv'\n",
    "df_red = pd.read_csv(df_path)\n",
    "\n",
    "ppt= participant.split('_')[1]\n",
    "participant_info = df_red[df_red['myphd_id'] == ppt]['record_id'].iloc[0]\n",
    "\n",
    "# Pattern to match QC_CHECK files\n",
    "pattern = os.path.join(base_dir, \"*.csv\")\n",
    "\n",
    "# Get a list of files that match the pattern\n",
    "qc_check_files = glob.glob(pattern)\n",
    "\n",
    "# Print the list of QC_CHECK files\n",
    "duration=0\n",
    "for file in qc_check_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Convert 'dateTime' column to datetime objects\n",
    "    df['dateTime'] = pd.to_datetime(df['dateTime'])\n",
    "\n",
    "    # Get duration\n",
    "    duration = df['dateTime'].max() - df['dateTime'].min()\n",
    "\n",
    "    # Get group\n",
    "    rand_group = df_red[df_red['myphd_id'] == ppt]['randomization_group'].values[0]\n",
    "\n",
    "    # Get HR zones\n",
    "    MICT_lower_val = df_red[df_red['myphd_id'] == ppt]['target_hr_45'].values[0]\n",
    "    MICT_upper_val = df_red[df_red['myphd_id'] == ppt]['target_hr_55'].values[0]\n",
    "    HIIT_lower_val = df_red[df_red['myphd_id'] == ppt]['target_hr_70'].values[0]\n",
    "\n",
    "    # Extract the time in HH:MM format\n",
    "    df['dateTime'] = df['dateTime'].dt.strftime('%H:%M')\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df['dateTime'], df['value'], label='Heart Rate')\n",
    "    plt.xlabel('Time (HH:MM)')\n",
    "    plt.ylabel('Heart Rate (bpm)')\n",
    "    plt.title(f'{duration}, {file.split(\"/\")[11]}')\n",
    "    plt.axhline(y=MICT_lower_val, color='g', linestyle='--')\n",
    "    plt.axhline(y=MICT_upper_val, color='y', linestyle='--')\n",
    "    plt.axhline(y=HIIT_lower_val, color='r', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
